{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordsegment import load, segment\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open('../Data/emnlp_dict.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "word_dict = {}\n",
    "\n",
    "# Loop through pairs and create a dictionary\n",
    "for pair in lines:\n",
    "    #remove \\n from the end of the line\n",
    "    pair = pair.strip()\n",
    "    words = pair.split('\\t')  # Split the pair by the tab character\n",
    "    if len(words) == 2:\n",
    "        word_dict[words[0]] = words[1]\n",
    "\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#picle the dictionary\n",
    "with open('../Data/emnlp_dict.pkl', 'wb') as file:\n",
    "    pkl.dump(word_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a string give the same string but all words in lowercase unless all the word is in uppercase\n",
    "def lower_case(text):\n",
    "    if text.isupper():\n",
    "        return text\n",
    "    else:\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case('HELLo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'YES HELLoooOOYESSsSIR'\n",
    "segmented = segment(word)\n",
    "print(segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_two_consecutive(string):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_two_consecutive('Yeeeeeeesss suuuuur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented2 = segment('GOOOOAAAAALLLLLL by Barcelona')\n",
    "print(segmented2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_word(word:str)->str:\n",
    "    if word in word_dict:\n",
    "        return word_dict[word]\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented3= segment('HELLOOOYESSSIR')\n",
    "segmented3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_word (text: str)-> list:\n",
    "    return segment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented4 = segment_word('HELLOOOYESSSIR')\n",
    "print(segmented4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_hashtag(text: str)-> str:\n",
    "    return re.sub(r'#', ' talking about ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hashtag(text: str)-> str:\n",
    "    \"\"\"\n",
    "    Splits the text by space and check if word starts with # and if it does it replaces it with talking about\n",
    "    and segment the word right after the #\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        if words[i].startswith('#'):\n",
    "            words[i] = 'talking about ' + ' '.join(word(words[i][1:]))\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'segment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m replace_hashtag(\u001b[39m'\u001b[39;49m\u001b[39mI am #happyday\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb Cell 18\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(words)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mif\u001b[39;00m words[i]\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m#\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         words[i] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtalking about \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(segment(words[i][\u001b[39m1\u001b[39m:]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(words)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'segment' is not defined"
     ]
    }
   ],
   "source": [
    "replace_hashtag('I am #happyday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" I am #happy    cde     cewbjk\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reasons', '2dothebirdmanhandrub']\n"
     ]
    }
   ],
   "source": [
    "txt = replace_hashtag('reasons2dothebirdmanhandrub')\n",
    "segmented5 = segment_word(txt)\n",
    "print(segmented5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = [\"studies\", 'studying', 'cries', 'cry']\n",
    "for w in text:\n",
    "    print(w, \" : \", wordnet_lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize_word(text: list)-> list:\n",
    "    return [wordnet_lemmatizer.lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_word([\"Don\\'t\", 'worry', \"not\" , 'be', 'happiness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segment 'I am sooooo #happy :D'\n",
    "\n",
    "segmented = segment('iamhappy !!?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_word(segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "change the emojis \n",
    "replace hashtag\n",
    "segment\n",
    "(penser a citer comment le dictionnaire de correction a ete fait)\n",
    "replace more than 2 consecutive letters by 2\n",
    "slang\n",
    "error correction\n",
    "lemmatize_word\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/tensorflow-macos/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aliessonni/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from TextProc.Preprocessor import Preprocessor\n",
    "from tqdm.notebook import tqdm as notebook_tqdm\n",
    "from TextProc.preproc_configs import preproc_config_context, preproc_config_nocontext\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_preprocessor = Preprocessor(preproc_config_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/twitter-datasets/train_pos.txt', 'r') as f:\n",
    "    train_pos = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_pos, columns=['text'])\n",
    "# keep only the first 1000 tweets\n",
    "df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; i dunno justin read my mention or not ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic is so dumb , i won't even c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" &lt;user&gt; just put casper in a box ! \" looved t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; thanks sir &gt; &gt; don't trip lil ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting my brother tmr is the bestest birthda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  <user> i dunno justin read my mention or not ....\n",
       "1  because your logic is so dumb , i won't even c...\n",
       "2  \" <user> just put casper in a box ! \" looved t...\n",
       "3  <user> <user> thanks sir > > don't trip lil ma...\n",
       "4  visiting my brother tmr is the bestest birthda..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_emoticons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 88106.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_two_consecutive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 250451.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_hashtag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 123093.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 255890.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string_to_list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1636482.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slang_to_english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1122371.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_word\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 731990.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_to_string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2024277.99it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed = tweet_preprocessor.preprocess(df)\n",
    "# write the df to a txt file\n",
    "preprocessed.to_csv('Data/preprocessed_train_pos_2.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\",\n",
    "    \"because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>, #happyyy\",\n",
    "    \"\\\" <user> just put casper in a box ! \\\" looved the battle ! #crakkbitch\",\n",
    "    \"<user> <user> thanks sir > > don't trip lil mama ... just keep doin ya thang !\",\n",
    "    \"<user> but seriously though .. it's called vanity fairest\",\n",
    "    \"<user> <user> lol chloe :') ill teach yous to cook , you'll be pros by the end of it !\",\n",
    "    \"<user> lol , im finna eat something ... chicken\",\n",
    "    \"any questions for me ? just put preston down for a short nap and mikes not home for another hour , feel like answer q's ! ! make ' em original\",\n",
    "    \"friday is payday & 4/20 ? ? hell yeah #reasons2dothebirdmanhandrub\"\n",
    "]\n",
    "\n",
    "tweet_df = pd.DataFrame(tweets, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_emoticons\n",
      "make_two_consecutive\n",
      "replace_hashtag\n",
      "remove_punctuation\n",
      "string_to_list\n",
      "correct_text\n",
      "slang_to_english\n",
      "correct_word\n",
      "list_to_string\n",
      "i I don't know justin read my mention or not only justin and god knows about that but i hope you will follow me talking about believe 15\n",
      "------------------\n",
      "because your logic is so dumb i won't even crop out your name or your photo tsk talking about happy\n",
      "------------------\n",
      "just put casper in a box loved the battle talking about Can't remember a okay bitch\n",
      "------------------\n",
      "thanks sir don't trip little mama pause just keep doing yeah thing\n",
      "------------------\n",
      "but seriously though it's called vanity fairest\n",
      "------------------\n",
      "laughing out loud chloe tears of joy ill teach yous to cook you'll be pros by the end of it\n",
      "------------------\n",
      "laughing out loud I am fitting to eat something pause chicken\n",
      "------------------\n",
      "any questions for me just put preston down for a short nap and mikes not home for another hour feel like answer q's make ' them original\n",
      "------------------\n",
      "friday is payday 4 20 hell yeah talking about reasons 2 do the birdman hand rub\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "preprocessed = tweet_preprocessor.preprocess(tweet_df)\n",
    "for text in preprocessed['text']:\n",
    "    print(text)\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me talking about believe 15\n",
      "------------------\n",
      "because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\n",
      "------------------\n",
      "\" <user> just put casper in a box ! \" looved the battle ! talking about cr a kk bitch\n",
      "------------------\n",
      "<user> <user> thanks sir > > don't trip lil mama ... just keep doin ya thang !\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for text in tweets:\n",
    "    print(replace_hashtag(text))\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crack', 'bitch']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wordninja.split('crackbitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am talking about happy day'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_hashtag('I am #happyday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demojizing...\n",
      "Replacing #...\n",
      "Replacing @...\n",
      "Translating slang...\n",
      "Segmenting...\n",
      "Correcting spelling...\n",
      "Demojizing...\n",
      "Replacing #...\n",
      "Replacing @...\n",
      "Translating slang...\n",
      "Segmenting...\n",
      "Correcting spelling...\n",
      "Demojizing...\n",
      "Replacing #...\n",
      "Replacing @...\n",
      "Translating slang...\n",
      "Segmenting...\n",
      "Correcting spelling...\n",
      "Demojizing...\n",
      "Replacing #...\n",
      "Replacing @...\n",
      "Translating slang...\n",
      "Segmenting...\n",
      "Correcting spelling...\n",
      "Demojizing...\n",
      "Replacing #...\n",
      "Replacing @...\n",
      "Translating slang...\n",
      "Segmenting...\n",
      "Correcting spelling...\n",
      "Demojizing...\n",
      "Replacing #...\n",
      "Replacing @...\n",
      "Translating slang...\n",
      "Segmenting...\n",
      "Correcting spelling...\n",
      "Demojizing...\n",
      "Replacing #...\n",
      "Replacing @...\n",
      "Translating slang...\n",
      "Segmenting...\n",
      "Correcting spelling...\n",
      "Demojizing...\n",
      "Replacing #...\n",
      "Replacing @...\n",
      "Translating slang...\n",
      "Segmenting...\n",
      "Correcting spelling...\n",
      "Demojizing...\n",
      "Replacing #...\n",
      "Replacing @...\n",
      "Translating slang...\n",
      "Segmenting...\n",
      "Correcting spelling...\n",
      "Processed Tweet 1: user i dunno justin read my mention or not  only justin and god knows about that  but i hope you will follow me talking about believe 15\n",
      "Processed Tweet 2: because your logic is so dumb  i wont even crop out your name or your photo  ask   talking about happy\n",
      "Processed Tweet 3:  user just put casper in a box   looked the battle  talking about crakkbitch\n",
      "Processed Tweet 4: user user thanks sir   dont trip lil mama pause just keep doin ya thang \n",
      "Processed Tweet 5: user but seriously though i its called vanity fairest\n",
      "Processed Tweet 6: user user lol chloe im ill teach you to cook  youll be pros by the end of it \n",
      "Processed Tweet 7: user lol  im finna eat something pause chicken\n",
      "Processed Tweet 8: any questions for me  just put preston down for a short nap and mikes not home for another hour  feel like answer es   make  em original\n",
      "Processed Tweet 9: friday is payday  420   hell yeah talking about reasons2dothebirdmanhandrub\n"
     ]
    }
   ],
   "source": [
    "from emoji import demojize\n",
    "from googletrans import Translator\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def preprocess_tweets(tweets):\n",
    "    processed_tweets = []\n",
    "\n",
    "    # Initialize the translator and spell checker\n",
    "    translator = Translator()\n",
    "    spell = SpellChecker()\n",
    "\n",
    "    for tweet in tweets:\n",
    "        # Replace emoticons with a short description\n",
    "        print('Demojizing...')\n",
    "        tweet = demojize(tweet)\n",
    "\n",
    "        print('Replacing #...')\n",
    "        # Replace # with a description like 'talking about'\n",
    "        tweet = re.sub(r'#', ' talking about ', tweet)\n",
    "\n",
    "        print('Replacing @...')\n",
    "        # Replace ... with a description like 'pause'\n",
    "        tweet = re.sub(r'\\.\\.\\.', ' pause ', tweet)\n",
    "\n",
    "        print('Translating slang...')\n",
    "        # Translate slang to English\n",
    "        tweet = translator.translate(tweet).text\n",
    "\n",
    "        print('Segmenting...')\n",
    "        # Correct all spelling mistakes\n",
    "        words = tweet.split()\n",
    "\n",
    "        print('Correcting spelling...')\n",
    "        corrected_words = [spell.correction(word) if spell.correction(word) != None else word for word in words]\n",
    "        tweet = ' '.join(corrected_words)\n",
    "\n",
    "        # Remove anything between < and >\n",
    "        tweet = re.sub(r'<.*?>', '', tweet)\n",
    "\n",
    "        # Remove punctuations\n",
    "        tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "\n",
    "        processed_tweets.append(tweet)\n",
    "\n",
    "    return processed_tweets\n",
    "\n",
    "# Input tweets\n",
    "tweets = [\n",
    "    \"<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\",\n",
    "    \"because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>, #happyyy\",\n",
    "    \"\\\" <user> just put casper in a box ! \\\" looved the battle ! #crakkbitch\",\n",
    "    \"<user> <user> thanks sir > > don't trip lil mama ... just keep doin ya thang !\",\n",
    "    \"<user> but seriously though .. it's called vanity fairest\",\n",
    "    \"<user> <user> lol chloe :') ill teach yous to cook , you'll be pros by the end of it !\",\n",
    "    \"<user> lol , im finna eat something ... chicken\",\n",
    "    \"any questions for me ? just put preston down for a short nap and mikes not home for another hour , feel like answer q's ! ! make ' em original\",\n",
    "    \"friday is payday & 4/20 ? ? hell yeah #reasons2dothebirdmanhandrub\"\n",
    "]\n",
    "\n",
    "# Preprocess tweets\n",
    "preprocessed_tweets = preprocess_tweets(tweets)\n",
    "\n",
    "# Display the preprocessed tweets\n",
    "for i, tweet in enumerate(preprocessed_tweets, start=1):\n",
    "    print(f\"Processed Tweet {i}: {tweet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb Cell 42\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m input_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThiss iss a testt sentennce withh sme mistakess and slanng.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m corrected_text \u001b[39m=\u001b[39m split_and_correct_advanced(input_text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(corrected_text)\n",
      "\u001b[1;32m/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb Cell 42\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m corrected_text \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# Check if the token is a word\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39;49mis_alpha:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39m# Check if the token is not a recognized word\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m spell_checker\u001b[39m.\u001b[39mcorrection(token\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mlower()) \u001b[39m==\u001b[39m token\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mlower():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m             \u001b[39m# If not, consider it as a candidate for correction\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aliessonni/Documents/GitHub/ml-project-2-oao/prpc.ipynb#X63sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m             corrected_text\u001b[39m.\u001b[39mappend(spell_checker\u001b[39m.\u001b[39mcorrection(token\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mlower()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_alpha'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spellchecker import SpellChecker  # You may need to install this package using: pip install pyspellchecker\n",
    "\n",
    "# Load the English NLP model from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Initialize the spell checker\n",
    "spell_checker = SpellChecker()\n",
    "\n",
    "def split_and_correct_advanced(text):\n",
    "    # Process the input text using spaCy\n",
    "    doc = nlp(text)\n",
    "    doc = str(doc).lower()\n",
    "    print(type(doc))\n",
    "\n",
    "    corrected_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        # Check if the token is a word\n",
    "        if token.is_alpha:\n",
    "            # Check if the token is not a recognized word\n",
    "            if not spell_checker.correction(token.text.lower()) == token.text.lower():\n",
    "                # If not, consider it as a candidate for correction\n",
    "                corrected_text.append(spell_checker.correction(token.text.lower()))\n",
    "            else:\n",
    "                corrected_text.append(token.text)\n",
    "        else:\n",
    "            corrected_text.append(token.text)\n",
    "\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Thiss iss a testt sentennce withh sme mistakess and slanng.\"\n",
    "corrected_text = split_and_correct_advanced(input_text)\n",
    "print(corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/twitter-datasets/train_pos.txt', 'r') as f:\n",
    "    train_pos = f.readlines()\n",
    "\n",
    "with open('Data/twitter-datasets/train_neg.txt', 'r') as f:\n",
    "    train_neg = f.readlines()\n",
    "\n",
    "import random\n",
    "# sample 1% of the data at random\n",
    "train_pos = random.sample(train_pos, int(len(train_pos) * 0.1))\n",
    "train_neg = random.sample(train_neg, int(len(train_neg) * 0.1))\n",
    "\n",
    "# write the sampled data to a txt file\n",
    "with open('Data/twitter-datasets/train_pos_sample.txt', 'w') as f:\n",
    "    f.writelines(train_pos)\n",
    "\n",
    "with open('Data/twitter-datasets/train_neg_sample.txt', 'w') as f:\n",
    "    f.writelines(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
