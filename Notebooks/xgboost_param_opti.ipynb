{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: nltk in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (3.8.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-macosx_10_9_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp39-cp39-macosx_10_9_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp39-cp39-macosx_10_9_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
      "  Downloading thinc-8.2.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp39-cp39-macosx_10_9_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.28.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy) (67.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: click in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2023.10.3)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.14.5-cp39-cp39-macosx_10_7_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Downloading blis-0.7.11-cp39-cp39-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Using cached confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Downloading spacy-3.7.2-cp39-cp39-macosx_10_9_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp39-cp39-macosx_10_9_x86_64.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp39-cp39-macosx_10_9_x86_64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp39-cp39-macosx_10_9_x86_64.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "Downloading pydantic_core-2.14.5-cp39-cp39-macosx_10_7_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp39-cp39-macosx_10_9_x86_64.whl (493 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.1/493.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.1-cp39-cp39-macosx_10_9_x86_64.whl (876 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m876.4/876.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp39-cp39-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "Using cached confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, pydantic-core, murmurhash, langcodes, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.5.2 pydantic-core-2.14.5 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install spacy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-md==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (67.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.24.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/omarbadri/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.1)\n",
      "Installing collected packages: en-core-web-md\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Classifiers...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/omarbadri/Documents/MA1/Machine Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb Cellule 1\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/omarbadri/Documents/MA1/Machine%20Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mClassifiers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mXGBoost\u001b[39;00m \u001b[39mimport\u001b[39;00m XGBoost\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/omarbadri/Documents/MA1/Machine%20Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#from TextProc.Preprocessor import Preprocessor\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/omarbadri/Documents/MA1/Machine%20Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mconfigs\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/omarbadri/Documents/MA1/Machine%20Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39margparse\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/omarbadri/Documents/MA1/Machine%20Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime\n",
      "File \u001b[0;32m~/Documents/MA1/Machine Learning/ML_course/ml-project-2-oao/configs.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mTextProc\u001b[39;00m \u001b[39mimport\u001b[39;00m preproc_configs\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mModels\u001b[39;00m \u001b[39mimport\u001b[39;00m model_configs\n\u001b[1;32m      5\u001b[0m configurations \u001b[39m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbert_config_train\u001b[39m\u001b[39m'\u001b[39m : {\n\u001b[1;32m      7\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmodel_config\u001b[39m\u001b[39m'\u001b[39m: model_configs\u001b[39m.\u001b[39mbert_base_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     },\n\u001b[1;32m     30\u001b[0m }\n",
      "File \u001b[0;32m~/Documents/MA1/Machine Learning/ML_course/ml-project-2-oao/TextProc/preproc_configs.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mTextProc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhelper\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m preproc_config_1 \u001b[39m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     identity\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m preproc_config_2 \u001b[39m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     remove_punctuation,\n\u001b[1;32m     10\u001b[0m     remove_stopwords,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     remove_whitespace\n\u001b[1;32m     14\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/MA1/Machine Learning/ML_course/ml-project-2-oao/TextProc/helper.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtextblob\u001b[39;00m \u001b[39mimport\u001b[39;00m TextBlob\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39men_core_web_md\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwordninja\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpkl\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     52\u001b[0m         name,\n\u001b[1;32m     53\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     54\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m     55\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m     56\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m     57\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "print('Loading Classifiers...')\n",
    "from Classifiers.Classifier import Classifier\n",
    "from Classifiers.XGBoost import XGBoost\n",
    "#from TextProc.Preprocessor import Preprocessor\n",
    "import configs\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as stats\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import nevergrad as ng\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Function to encode text data using BERT\n",
    "def bert_encode(texts, tokenizer, model, max_len=512):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for text in texts:\n",
    "            encoded = tokenizer.encode_plus(\n",
    "                text,\n",
    "                max_length=max_len,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "        # Extract the tokens' embeddings\n",
    "        features = last_hidden_states[0][:,0,:].numpy()\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEngine():\n",
    "    def __init__(self, train, config, train_data_path, save_path):\n",
    "        self.train = train\n",
    "        self.config = config\n",
    "        self.train_data_path = train_data_path\n",
    "        self.save_model_path = save_path\n",
    "\n",
    "        # intialize classifier and preprocessor\n",
    "        self.model = self.choose_model(config['model_config'])\n",
    "        #self.preprocessor = Preprocessor(config['preproc_config'])\n",
    "\n",
    "\n",
    "    def load_data(self, data_path: str, train: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        loads data from a csv file\n",
    "        :param path: path to csv file\n",
    "        :return: pandas dataframe\n",
    "        \"\"\"\n",
    "        # load the dataset of texts one text per line\n",
    "        pos_path = '{}/train_pos_preprocessed.txt'.format(data_path)\n",
    "        neg_path = '{}/train_neg_preprocessed.txt'.format(data_path)\n",
    "        test_path = '{}/test_data.txt'.format(data_path)\n",
    "\n",
    "        if train:\n",
    "            with open(pos_path, 'r') as f:\n",
    "                train_pos = f.readlines()\n",
    "\n",
    "            with open(neg_path.format(data_path), 'r') as f:\n",
    "                train_neg = f.readlines()\n",
    "\n",
    "            # create a dataframe with the text and label\n",
    "            df = pd.DataFrame({'text': np.concatenate([train_pos, train_neg]),\n",
    "                                'label': np.concatenate([np.ones(len(train_pos)), np.zeros(len(train_neg))])\n",
    "                                })\n",
    "        else:\n",
    "            with open(test_path, 'r') as f:\n",
    "                test = f.readlines()\n",
    "            df = pd.DataFrame({'text': test})\n",
    "            # split text on the first comma\n",
    "            df['Id'] = df['text'].apply(lambda x: x.split(',', 1)[0])\n",
    "            df['text'] = df['text'].apply(lambda x: x.split(',', 1)[1])\n",
    "        \n",
    "        print('size of training data:', df.shape)\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def save_metrics(self, metrics: dict, path: str) -> None:\n",
    "        \"\"\"\n",
    "        saves metrics to a csv file\n",
    "        :param metrics: metrics to be saved\n",
    "        :param path: path to csv file\n",
    "        \"\"\"\n",
    "        pd.DataFrame.from_dict(metrics, orient='index').to_csv(path)\n",
    "\n",
    "    def save_predictions(self, predictions: list, path: str) -> None:\n",
    "        \"\"\"\n",
    "        saves predictions to a csv file\n",
    "        :param predictions: predictions to be saved\n",
    "        :param path: path to csv file\n",
    "        \"\"\"\n",
    "        pd.DataFrame(predictions).to_csv(path)\n",
    "\n",
    "    def choose_model(self, model_config: dict) -> Classifier:\n",
    "        \"\"\"\n",
    "        chooses a model based on the model_type\n",
    "        :param model_type: type of model to be chosen\n",
    "        :return: model\n",
    "        \"\"\"\n",
    "        # if model_config['model_type'] == 'bert':\n",
    "            # return BERT(model_config)\n",
    "        if model_config['model_type'] == 'xgboost':\n",
    "            return XGBoost(model_config)\n",
    "        # elif model_config['model_type'] == 'xlnet':\n",
    "        #     # add item to model_config\n",
    "        #     mc = deepcopy(model_config)\n",
    "        #     mc['output_dir'] = self.save_model_path\n",
    "        #     mc['best_model_dir'] = self.save_model_path + '/best_model/'\n",
    "        #     mc['cache_dir'] = self.save_model_path + '/cache_dir/'\n",
    "        #     mc['tensorboard_dir'] = self.save_model_path + '/tensorboard_dir/'\n",
    "        #     return XLNet(mc)\n",
    "        else:\n",
    "            raise ValueError('Model type not supported')\n",
    "        \n",
    "    def perform_random_search(self, X_train, y_train, param_dist, n_iter=10, cv=5, scoring='accuracy'):\n",
    "        \"\"\"\n",
    "        Perform random search for hyperparameter tuning.\n",
    "        :param X_train: Training features\n",
    "        :param y_train: Training labels\n",
    "        :param param_dist: Dictionary with parameters names as keys and distributions or lists of parameters to try.\n",
    "        :param n_iter: Number of parameter settings that are sampled.\n",
    "        :param cv: Number of folds in cross-validation.\n",
    "        :param scoring: Strategy to evaluate the performance of the cross-validated model on the test set.\n",
    "        :return: Fitted RandomizedSearchCV object\n",
    "        \"\"\"\n",
    "        if not isinstance(self.model, XGBoost):\n",
    "            raise ValueError(\"Random search is currently only implemented for XGBoost models\")\n",
    "\n",
    "        random_search = RandomizedSearchCV(self.model.xgb_classifier, param_distributions=param_dist, \n",
    "                                           n_iter=n_iter, cv=cv, scoring=scoring, verbose=1, random_state= 42)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        return random_search\n",
    "\n",
    "    def print_metrics(self, metrics: dict) -> None:\n",
    "        \"\"\"\n",
    "        prints the metrics\n",
    "        :param metrics: metrics to be printed\n",
    "        \"\"\"\n",
    "        for key, value in metrics.items():\n",
    "            print('{}: {}'.format(key, value))\n",
    "            \n",
    "    def create_xgboost_model(self, hyperparams):\n",
    "        \"\"\"\n",
    "        Create a new instance of the XGBoost model with given hyperparameters.\n",
    "        \"\"\"\n",
    "        config = self.config['model_config']\n",
    "        config.update(hyperparams)\n",
    "        return XGBoost(config)\n",
    "            \n",
    "            \n",
    "    def objective_function(self, hyperparams):\n",
    "        \"\"\"\n",
    "        Objective function for Nevergrad optimization.\n",
    "        \"\"\"\n",
    "        # Create a new XGBoost model instance with the given hyperparameters\n",
    "        model = self.create_xgboost_model(hyperparams)\n",
    "\n",
    "        # Train the model\n",
    "        model.train(self.X_train, self.y_train)\n",
    "\n",
    "        # Validate the model\n",
    "        predictions = model.predict(self.X_val)\n",
    "\n",
    "        # Calculate and return the negative accuracy (since Nevergrad minimizes)\n",
    "        accuracy = accuracy_score(self.y_val, predictions)\n",
    "        return -accuracy\n",
    "    \n",
    "    def optimize_hyperparameters(self):\n",
    "        \"\"\"\n",
    "        Use Nevergrad to optimize hyperparameters.\n",
    "        \"\"\"\n",
    "        # Define the hyperparameter space\n",
    "        parametrization = ng.p.Dict(\n",
    "            learning_rate=ng.p.Scalar(lower=0.01, upper=0.2).set_mutation(sigma=0.01),\n",
    "            max_depth=ng.p.Scalar(lower=3, upper=10).set_integer_casting(),\n",
    "            n_estimators=ng.p.Scalar(lower=100, upper=1000).set_integer_casting(),\n",
    "            subsample=ng.p.Scalar(lower=0.5, upper=1)\n",
    "        )\n",
    "\n",
    "        # Choose the optimizer\n",
    "        optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=100)\n",
    "\n",
    "        # Run the optimization\n",
    "        for _ in range(optimizer.budget):\n",
    "            x = optimizer.ask()\n",
    "            loss = self.objective_function(x.value)\n",
    "            optimizer.tell(x, loss)\n",
    "\n",
    "        # Get the best hyperparameters\n",
    "        best_hyperparams = optimizer.provide_recommendation().value\n",
    "\n",
    "        # Evaluate the objective function with the best hyperparameters\n",
    "        best_score = self.objective_function(best_hyperparams)\n",
    "\n",
    "        # Convert the score to a positive value, if necessary\n",
    "        best_accuracy = -best_score if best_score < 0 else best_score\n",
    "\n",
    "        return best_hyperparams, best_accuracy\n",
    "    \n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        print('Loading data...')\n",
    "        X = self.load_data(self.train_data_path, train=self.train)\n",
    "        print(X.head())\n",
    "        if self.train:\n",
    "            # load data\n",
    "            y = X['label']\n",
    "            # drop label column\n",
    "            X.drop('label', axis=1, inplace=True)\n",
    "\n",
    "            print('Preprocessing data...')\n",
    "            # preprocess data\n",
    "            #X = self.preprocessor.preprocess(X)\n",
    "            \n",
    "            print(\"Type of X:\", type(X))\n",
    "\n",
    "            \n",
    "            text_column = X['text']\n",
    "\n",
    "            # Now apply TF-IDF Vectorization\n",
    "            vectorizer = TfidfVectorizer(max_features=5000)\n",
    "            X_vectorized = vectorizer.fit_transform(text_column)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #X_vectorized = bert_encode(text_column, tokenizer, model)\n",
    "         \n",
    "            \n",
    "            # split data into train and validation\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "            # convert to list\n",
    "            y_train = np.array(y_train.tolist())\n",
    "            y_val = np.array(y_val.tolist())\n",
    "            y_val = np.array([-1 if y == 0 else 1 for y in y_val])\n",
    "            \n",
    "            param_dist = {\n",
    "                'max_depth': stats.randint(3, 10),\n",
    "                'learning_rate': stats.uniform(0.01, 0.1),\n",
    "                'subsample': stats.uniform(0.5, 0.5),\n",
    "                'n_estimators':stats.randint(50, 200)\n",
    "            }\n",
    "            \n",
    "            print(X_train.shape)\n",
    "            print(X_val.shape)\n",
    "            \n",
    "            # Define the hyperparameter grid\n",
    "            param_grid = {\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.1, 0.01, 0.001],\n",
    "                'subsample': [0.5, 0.7, 1]\n",
    "            }\n",
    "\n",
    "            # print('Performing Random Search for Hyperparameter Tuning...')\n",
    "\n",
    "            # random_search_result = self.perform_random_search(X_train, y_train, param_dist)\n",
    "            \n",
    "            # print(\"Best set of hyperparameters: \", random_search_result.best_params_)\n",
    "            # print(\"Best score: \", random_search_result.best_score_)\n",
    "            \n",
    "            params = {\n",
    "                'learning_rate': 0.06986584841970366,\n",
    "                'max_depth': 9,\n",
    "                'n_estimators': 171,\n",
    "                'subsample': 0.5779972601681014\n",
    "            }\n",
    "            \n",
    "            self.X_train, self.X_val, self.y_train, self.y_val = X_train, X_val, y_train, y_val\n",
    "\n",
    "\n",
    "            #print('Optimizing hyperparameters...')\n",
    "            #best_hyperparams, best_accuracy = self.optimize_hyperparameters()\n",
    "            #print('Best hyperparameters:', best_hyperparams)\n",
    "            #print('Best accuracy:', best_accuracy)\n",
    "\n",
    "            # Set the best hyperparameters to the model\n",
    "            #self.model.set_params(**best_hyperparams)\n",
    "            \n",
    "\n",
    "            print('Training...')\n",
    "            # train\n",
    "            self.model.train(X_train, y_train)\n",
    "            # dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            # self.model = xgb.train(params, dtrain)\n",
    "\n",
    "            print('Saving model...')\n",
    "            # save model\n",
    "            self.model.save(self.save_model_path)\n",
    "\n",
    "            print('Testing...')\n",
    "            # test\n",
    "            self.model.validate(X_val, y_val)\n",
    "\n",
    "            print('Metrics:')\n",
    "            # get metrics\n",
    "            metrics = self.model.get_metrics()\n",
    "            self.print_metrics(metrics)\n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            # load data\n",
    "            X = self.load_data(self.train_data_path, train=False)\n",
    "            # preprocess data\n",
    "            X['text'] = self.preprocessor.preprocess(X['text'].to_numpy())\n",
    "            # convert to list\n",
    "            X_val = X['text'].to_numpy()\n",
    "\n",
    "            print('Predicting...')\n",
    "            # predict\n",
    "            X['label'] = self.model.predict(X_val)\n",
    "\n",
    "            print('Saving predictions...')\n",
    "            # save predictions\n",
    "            self.save_predictions(X[['Id', 'Prediction']], self.save_model_path)\n",
    "\n",
    "        print('Finished Running !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current date time...\n",
      "Parsing arguments...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/omarbadri/Documents/MA1/Machine Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb Cellule 4\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/omarbadri/Documents/MA1/Machine%20Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Instead of argparse, directly assign values to variables\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/omarbadri/Documents/MA1/Machine%20Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_config \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mxgboost_config_train\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/omarbadri/Documents/MA1/Machine%20Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m config \u001b[39m=\u001b[39m configs\u001b[39m.\u001b[39mget_config(train_config)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/omarbadri/Documents/MA1/Machine%20Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/omarbadri/Documents/MA1/Machine%20Learning/ML_course/ml-project-2-oao/xgboost_test3.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_data_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mData/twitter-datasets\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'configs' is not defined"
     ]
    }
   ],
   "source": [
    "print('Getting current date time...')\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%m-%d-%Y-%H:%M:%S\")\n",
    "\n",
    "print('Parsing arguments...')\n",
    "# Instead of argparse, directly assign values to variables\n",
    "train_config = 'xgboost_config_train'\n",
    "config = configs.get_config(train_config)\n",
    "train = True\n",
    "train_data_path = 'Data/twitter-datasets'\n",
    "save_model_path = f'Models/model_{current_time}'\n",
    "\n",
    "print('Loading config...')\n",
    "config = configs.get_config(train_config)\n",
    "\n",
    "print('Initializing train engine...')\n",
    "engine = TrainEngine(train, config, train_data_path, save_model_path)\n",
    "\n",
    "print('Running training...')\n",
    "engine.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
