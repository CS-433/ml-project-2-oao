{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install textblob\n",
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_md\n",
    "! pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/tensorflow-macos/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aliessonni/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from TextProc.Preprocessor import Preprocessor\n",
    "from TextProc.preproc_configs import preproc_config_nocontext, preproc_config_context\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the text files from 'Data/twitter-datasets'\n",
    "with open('Data/twitter-datasets/train_pos/full.txt', 'r') as f:\n",
    "    train_pos = f.readlines()\n",
    "with open('Data/twitter-datasets/train_neg/full.txt', 'r') as f:\n",
    "    train_neg = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \\n from the end of each line\n",
    "train_pos = [line[:-1] for line in train_pos]\n",
    "train_neg = [line[:-1] for line in train_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of positive tweet given '#': 0.5893756613756613\n",
      "Probability of negative tweet given '#': 0.4106243386243386\n"
     ]
    }
   ],
   "source": [
    "total_positive_tweets = len(train_pos)\n",
    "total_negative_tweets = len(train_neg)\n",
    "\n",
    "word = \"#\"  # Replace \"user\" with the specific word you're interested in\n",
    "positive_tweets_with_user = sum(1 for tweet in train_pos if word in tweet.lower())\n",
    "negative_tweets_with_user = sum(1 for tweet in train_neg if word in tweet.lower())\n",
    "\n",
    "# Calculate probabilities\n",
    "prob_positive_user = positive_tweets_with_user / (positive_tweets_with_user+negative_tweets_with_user)\n",
    "prob_negative_user = negative_tweets_with_user / (positive_tweets_with_user+negative_tweets_with_user)\n",
    "\n",
    "print(f\"Probability of positive tweet given '{word}': {prob_positive_user}\")\n",
    "print(f\"Probability of negative tweet given '{word}': {prob_negative_user}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(preproc_config_nocontext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_emoticons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 95292.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_unmatched_parentheses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 263115.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_two_consecutive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 205626.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_hashtag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 154499.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 246099.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decontracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 252125.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string_to_list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 1106297.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 398565.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_to_string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 1584550.06it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('Data/twitter-datasets/test/test_data.txt', 'r') as f:\n",
    "    test_df = f.readlines()\n",
    "\n",
    "test_df = [line[:-1] for line in test_df]\n",
    "\n",
    "# split each tweet into first part before the first comma and second part after the first comma\n",
    "test_df = [line.split(',', 1) for line in test_df]\n",
    "# create a dataframe from the list of lists\n",
    "test_df = pd.DataFrame(test_df, columns=['Id', 'text'])\n",
    "# apply the preprocessor to the text column\n",
    "test_df = preprocessor.preprocess(test_df)\n",
    "\n",
    "# add a '\\n' to the end of each tweet\n",
    "test_df['text'] = test_df['text'].apply(lambda x: x+'\\n')\n",
    "\n",
    "# convert to list of lists\n",
    "test_df = test_df.values.tolist()\n",
    "# write each line to a seperate line in a txt file with the same format as the train data\n",
    "with open('Data/twitter-datasets/test/test_context.txt', 'w') as f:\n",
    "    for line in test_df:\n",
    "        f.write(','.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_emoticons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:11<00:00, 108322.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_unmatched_parentheses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:04<00:00, 310445.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_two_consecutive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:04<00:00, 271858.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_hashtag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:07<00:00, 169548.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:04<00:00, 278686.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decontracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:04<00:00, 287333.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string_to_list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:02<00:00, 523939.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:13<00:00, 93459.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:02<00:00, 512322.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatize_word\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:14<00:00, 86300.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_to_string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:00<00:00, 2234166.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_emoticons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:11<00:00, 105613.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_unmatched_parentheses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:05<00:00, 247753.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_two_consecutive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:05<00:00, 231135.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace_hashtag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:05<00:00, 223683.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:05<00:00, 246241.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decontracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:04<00:00, 261234.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string_to_list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:02<00:00, 456461.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:15<00:00, 79177.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:03<00:00, 404577.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatize_word\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:20<00:00, 61408.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_to_string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250000/1250000 [00:00<00:00, 1985637.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# transform arrays to dataframes\n",
    "train_pos_df = pd.DataFrame(train_pos, columns=['text'])\n",
    "train_neg_df = pd.DataFrame(train_neg, columns=['text'])\n",
    "\n",
    "# preprocess the dataframes\n",
    "train_pos_df = preprocessor.preprocess(train_pos_df)\n",
    "train_neg_df = preprocessor.preprocess(train_neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append \\n to the end of each line\n",
    "train_pos_df['text'] = train_pos_df['text'].apply(lambda x: x + '\\n')\n",
    "train_neg_df['text'] = train_neg_df['text'].apply(lambda x: x + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# write the datasets to txt files\n",
    "with open('Data/twitter-datasets/train_pos/full_nocontext.txt', 'w') as f:\n",
    "    f.writelines(train_pos_df['text'].tolist())\n",
    "with open('Data/twitter-datasets/train_neg/full_nocontext.txt', 'w') as f:\n",
    "    f.writelines(train_neg_df['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    removes all punctuation from a string\n",
    "    :param text: text to remove punctuation from\n",
    "    :return: text without punctuation\n",
    "    \"\"\"\n",
    "    # replace all punctiation except ' with a space\n",
    "    text = text.translate(str.maketrans(string.punctuation.replace('\\'', ''), ' ' * (len(string.punctuation) - 1)))\n",
    "    # remove double spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextProc.helper import correct_text, replace_hashtag, replace_emoticons, make_two_consecutive\n",
    "from TextProc.Preprocessor import Preprocessor\n",
    "from TextProc.preproc_configs import preproc_config_context\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation_words = [\n",
    "    \"not\", \"no\", \"never\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nor\", \"nowhere\",\n",
    "    \"hardly\", \"scarcely\", \"barely\", \"rarely\", \"seldom\", \"little\", \"don't\", \"can't\",\n",
    "    \"won't\", \"isn't\", \"aren't\", \"couldn't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\",\n",
    "    \"hadn't\", \"shouldn't\", \"wouldn't\", \"mustn't\", \"mightn't\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_tweet(tweet, words):\n",
    "    return any(word in tweet.lower() for word in words)\n",
    "\n",
    "# Calculating probabilities for target words\n",
    "total_pos = len(train_pos_df)\n",
    "total_neg = len(train_neg_df)\n",
    "tweets_with_target_pos = train_pos_df['text'].apply(lambda tweet: word_in_tweet(tweet, negation_words)).sum()\n",
    "tweets_with_target_neg = train_neg_df['text'].apply(lambda tweet: word_in_tweet(tweet, negation_words)).sum()\n",
    "\n",
    "# Probability calculations\n",
    "prob_target_pos = tweets_with_target_pos / (tweets_with_target_pos + tweets_with_target_neg)\n",
    "prob_target_neg = tweets_with_target_neg / (tweets_with_target_pos + tweets_with_target_neg)\n",
    "\n",
    "print(f\"Probability of positive tweet given target word: {prob_target_pos}\")\n",
    "print(f\"Probability of negative tweet given target word: {prob_target_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_unmatched_left_parentheses(text: str) -> bool:\n",
    "    stack = []\n",
    "    dummy = 0 \n",
    "\n",
    "    for char in text:\n",
    "        if char == '(':\n",
    "            stack.append(char)\n",
    "        elif char == ')':\n",
    "            if len(stack) > 0:\n",
    "                stack.pop()\n",
    "            else:\n",
    "                dummy+=1  # Unmatched right parenthesis found\n",
    "        # Ignore other characters\n",
    "    \n",
    "    return 1 if len(stack) > 0 else 0 # Unmatched left parentheses found at the end\n",
    "\n",
    "# Example usage:\n",
    "text_example = \"((Hello))))(\"\n",
    "result = has_unmatched_left_parentheses(text_example)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#has unmatched right parentheses\n",
    "def has_unmatched_right_parentheses(text: str) -> bool:\n",
    "    stack = []\n",
    "    dummy = 0 \n",
    "\n",
    "    for char in text:\n",
    "        if char == '(':\n",
    "            stack.append(char)\n",
    "        elif char == ')':\n",
    "            if len(stack) > 0:\n",
    "                stack.pop()\n",
    "            else:\n",
    "                dummy+=1  # Unmatched right parenthesis found\n",
    "        # Ignore other characters\n",
    "    \n",
    "    return int(dummy>0) # Unmatched left parentheses found at the end\n",
    "\n",
    "text_example = \"((Hello)()))))\"\n",
    "result = has_unmatched_right_parentheses(text_example)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating probabilities for '<user>' and '<url>'\n",
    "total_tweets_pos = len(train_pos_df)\n",
    "tweets_with_neg = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_unbalanced_l_count = train_pos_df['text'].apply(lambda tweet: has_unmatched_left_parentheses(tweet)).sum()\n",
    "negative_unbalanced_l_count = train_neg_df['text'].apply(lambda tweet: has_unmatched_left_parentheses(tweet)).sum()\n",
    "positive_unbalanced_r_count = train_pos_df['text'].apply(lambda tweet: has_unmatched_right_parentheses(tweet)).sum()\n",
    "negative_unbalanced_r_count = train_neg_df['text'].apply(lambda tweet: has_unmatched_right_parentheses(tweet)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pos_unbalanced_l = positive_unbalanced_l_count / (positive_unbalanced_l_count + negative_unbalanced_l_count)\n",
    "p_neg_unbalanced_l = negative_unbalanced_l_count / (positive_unbalanced_l_count + negative_unbalanced_l_count)\n",
    "p_pos_unbalanced_r = positive_unbalanced_r_count / (positive_unbalanced_r_count + negative_unbalanced_r_count)\n",
    "p_neg_unbalanced_r = negative_unbalanced_r_count / (positive_unbalanced_r_count + negative_unbalanced_r_count)\n",
    "\n",
    "print(f\"Probability of positive tweet given unbalanced left parenthesis: {p_pos_unbalanced_l}\")\n",
    "print(f\"Probability of negative tweet given unbalanced left parenthesis: {p_neg_unbalanced_l}\")\n",
    "print(f\"Probability of positive tweet given unbalanced right parenthesis: {p_pos_unbalanced_r}\")\n",
    "print(f\"Probability of negative tweet given unbalanced right parenthesis: {p_neg_unbalanced_r}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
